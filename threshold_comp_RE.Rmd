---
title: "Threshold girdle"
author: "Chloe Brown & John Kirwan"
date: '2020-10-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library('sciplot')
library('multcomp')
library('ade4')
library('vegan')
library('ggplot2')
library('lme4')
library('tidyverse')
library('ggeffects')
library('AICcmodavg') # Used for AIC comparison
```


```{r}
thresholdcomp              <- read_csv("thresholdcomp.csv")
thresholdcomp$treatment    <- as.factor(thresholdcomp$treatment)
thresholdcomp$size         <- as.factor(thresholdcomp$size)
thresholdcomp$individual   <- as.factor(thresholdcomp$individual)
thresholdcomp$response     <- as.factor(thresholdcomp$response)
str(thresholdcomp)
```


### Graphs

```{r}
stimmult <-thresholdcomp$response*100
stimorder <-factor(thresholdcomp$stimulus, levels = unique(thresholdcomp$stimulus)) 
# to make bars in the order found in csv file

bargraph.CI(stimorder, stimmult, group = treatment, legend=T,
            data=thresholdcomp, ylim=c(0,100), xlab = "Stimulus",
            ylab = "Shadow Responses Elicited (%)")
```

Still need to find a way to make this plot nicer.

```{r}
thresholdcomp %>% 
  ggplot(aes(x = factor(stimulus),fill = response)) + 
    geom_bar(position = "fill")
```

### Stripchart

```{r}
props <- with(thresholdcomp, aggregate(response, by  = list(length = length, stimulus = stimulus), mean))

#stripchart(x~stimulus*length, data = props, vertical = T, cex = 0.5, ylab = 'Proportion Responding')
```

### Likelihood models with lme4

If it says singular thing then it means there is not enough infomration because you have split the data up too much with effects. If model not working, try changing optimisation.

#### Main model

```{r Main model}
compmodel.5<- glmer(
  response ~ stimulus + treatment + (1|individual)  + (1|stim_order), # + (1|length)
  data = thresholdcomp, family = binomial("logit"),
  control = glmerControl(optimizer = "optimx", calc.derivs = FALSE,
                         optCtrl = list(method = "nlminb", starttests = FALSE))) 

compmodel.5
```


#### Null model without stimulus

```{r Null model}
compmodel.null <-glmer(
  response ~ treatment + (1|individual)  + (1|stim_order), # + (1|length)
  data = thresholdcomp, family = binomial(), 
  control = glmerControl(optimizer = "optimx", calc.derivs = FALSE,
            optCtrl = list(method = "nlminb", starttests = FALSE)))

compmodel.null
```


#### Null model without stimulus or treatment

```{r}
compmodel.0 <-glmer(
  response ~ 1  + (1|individual) + (1|stim_order), #  + (1|length)
  data = thresholdcomp, family = binomial(), 
  control = glmerControl(optimizer = "optimx", calc.derivs = FALSE,
            optCtrl = list(method = "nlminb", starttests = FALSE)))

compmodel.0
```


#### Full model with stimulus, treatment, and their interaction

```{r}
compmodel.int <-glmer(
  response ~ stimulus*treatment  + (1|individual)  + (1|stim_order), #+ (1|length)
  data = thresholdcomp, family = binomial(),
  control = glmerControl(optimizer = "optimx", calc.derivs = FALSE,
             optCtrl = list(method = "nlminb", starttests = FALSE)))

compmodel.int
```

        
#### Full model with stimulus, treatment, and their interaction and random slopes
                                             
```{r}
compmodel.intslope <-glmer(
  response ~ stimulus*treatment  + (1+stimulus|individual)  + (1|stim_order), #+ (1+stimulus|length)
  data = thresholdcomp, family = binomial(),
  control = glmerControl(optimizer = "optimx", calc.derivs = FALSE,
                optCtrl = list(method = "nlminb", starttests = FALSE))) 

compmodel.intslope
```

                                             
                                             
#putting (1+stimulus|random) effect tells the model that there may be differing intercepts based on the random effect, i.e. the baseline responses of the individual and those of differing sizes may be different.
#I didn't add to stim_order because it threw an error.


```{r}
anova(compmodel.5, compmodel.null,compmodel.0,compmodel.int, compmodel.intslope) 
```



```{r}
models <- list(compmodel.5, compmodel.null,compmodel.0,compmodel.int, compmodel.intslope)
model.names <- c('compmodel.5', 'compmodel.null','compmodel.0','compmodel.int', 'compmodel.intslope')
aictab(cand.set = models, modnames = model.names)
```

Based on this, the best model is the one with an interaction between stimulus and treatment that allow for different baseline responses in length and individual. 

> It carries 100% of total explanation.

What do you mean by that last part? 

#### Look at the model summary

```{r}
summary(compmodel.intslope)
```

### Chloe's comments

> If this all looks OK, then please can I have some help interpreting the output of this.

Yes. This looks OK. And yes, I think i can help.

> What I think I can say: there's an effect of treatment because there the best model included an interaction between the stimuli the individual has a strong (I think) effect and length does but less so.

Yes. It is the best model, it also has far more degrees of freedom than the others but it is much better regardless, as revealed by the AIC values and the LRT tests.

Yes, the effect of individual is large, in relation to how the response changed with stimulus (it makes sense that it should be conditional upon stimulus - they should all do equally poorly when there is no stimulus). Did you observe this? Did different animals perform very differently? You can also make a summary plot of the original data, which plots a different line for each individual to test this. 

Yes, the effect is length is much smaller. If you wanted, you could also filter your data to include only large, salient stimuli and plot the proportion responses against body length.

> I'm not sure how to interpret the intercept and stimulus part.

The intercept and slope (stimulus) are the parameters of the regression line but they are still in logit space. You can get the model fit by plugging them back into the line formula (y=mx+c) and putting it into logit space with the logit link, which in R, you can apply with pnorm().

```{r}
fixed <- fixef(compmodel.intslope) 
fixed
```

Above, we pull out the fixed effect coefficients. 

```{r}
X  <- seq(from=-1,to=0.2,by=.02)
y_control <- pnorm( fixed[2]*X + fixed[1])
plot(X,y_control,main='Whole (control) stimulus')
lines(X,y_control)
```

The intercept represents the *whole* treatment, so that is what is plotted above. How does this compare to your data?

You can compare how to predictions out of the model (below), though I think this should be across all treatments. 


```{r}
x_prediction_sequence <- seq(from=-1, to=0.2, by= 0.005)

predz <- ggpredict(
  compmodel.intslope, 
  terms = c('stimulus [x_prediction_sequence]','treatment'),
  type='fixed', # can also be random
  ci.lvl = 0.95, # 1- alpha (usually 0.05) level of confidence intervals
                   ) 
predz %>% plot(colors=c("blue","black","orange"))
```

> I don't know what's going on with the correlation of fixed effects or the fixed effects.

Don't worry so much about the correlations. That is literally telling you if they are correlated (it's a covariance matrix) which can be important, for instance if you have highly correlated effects effects which can be problematic. 

The values out of the fix effects are *contrasts* contrasted against the default situation, which is the intercept (decided by the order of your factor levels). To get other values, you add the effects; e.g. to get the response in relation to stimulus with the valve stimulus, you can get. Again, you can compare to the panel above.

```{r}
plot(X,y_control,main='Response to stimuli',col="blue",xlim=c(-1,0.2))
lines(X,y_control,col="blue")

y_valve <- pnorm( (fixed[2] + fixed[6])*X + (fixed[1] + fixed[4]) )
points(X,y_valve,col="orange")
lines(X,y_valve,col="orange")

y_girdle <- pnorm( (fixed[2] + fixed[5])*X + (fixed[1] + fixed[3]) )
points(X,y_girdle,col="black")
lines(X,y_girdle,col="black")

abline(v=0,col="grey")
```

I made the primitive plot above so that you can see how the fancier plot is made. The fancier plot also includes the measures of error. 


### Building the graph with predz and means of each



```{r}
controlpredz <- subset(predz, group == "Control")
girdlepredz <- subset(predz, group == "Girdle")
valvepredz <- subset(predz, group == "Valve")

xgirdle <- girdlepredz$x
ygirdle <- girdlepredz$predicted
girdlemin <- girdlepredz$conf.low
girdlemax <- girdlepredz$conf.high
xmgirdle <- c(-0.97,-0.96,-0.9,-0.73,-0.52,-0.26,-0.03,0) # stimulus treatments. Girdle has an extra one -0.73 compared to the others.
ymgirdle <- c(0.95986622, 0.81012658, 0.77021277, 0.66530612, 0.52343750, 0.17241379, 0.02348993, 0.01672241) #mean response rates

xvalve <-valvepredz$x
yvalve <- valvepredz$predicted
valvemin <- valvepredz$conf.low
valvemax <- valvepredz$conf.high
xmvalve <- c(-0.97,-0.96, -0.9,-0.52, -0.26, -0.03, 0)
ymvalve <- c(0.97765363,0.91304348,0.88554217,0.86206897, 0.75428571, 0.10169492, 0.05555556)

xwhole <- controlpredz$x
ywhole <- controlpredz$predicted
wholemin <- controlpredz$conf.low
wholemax <- controlpredz$conf.high
xmwhole <-c(-0.97, -0.96, -0.9, -0.52, -0.26, -0.03, 0) 
ymwhole <- c(0.96111111, 0.98333333, 0.97777778, 0.95555556, 0.96111111, 0.36111111, 0.03333333)
```




```{r}
ggplot() + 
  geom_line(data = girdlepredz, aes(xgirdle,ygirdle), colour = "black", size = 1) + 
  geom_ribbon(aes(x = xgirdle, y=ygirdle,ymin=girdlemin, ymax=girdlemax),
              linetype=2, alpha=0.1, fill = "black") +
  geom_point(aes(x = xmgirdle, y = ymgirdle), colour = "black", size = 3, pch=0) +
  geom_vline(aes(xintercept = -0.574), colour = "black") +
  geom_line(data = valvepredz, aes(xvalve,yvalve), colour = "orange", size = 1) +
  geom_point(aes(x = xmvalve, y = ymvalve), colour = "orange", size = 3, pch=1) +
  geom_ribbon(aes(x = xvalve, y=yvalve,ymin=valvemin, ymax=valvemax),
              linetype=2, alpha=0.1, fill = "orange") +
  geom_vline(aes(xintercept = -0.206), colour = "orange") +
  geom_line(data = controlpredz, aes(xwhole,ywhole), colour = "blue", size = 1) +
  geom_point(aes(x = xmwhole, y = ymwhole), colour = "blue", size = 3, pch=2) +
  geom_ribbon(aes(x = xwhole, y=ywhole,ymin=wholemin, ymax=wholemax),
              linetype=2, alpha=0.1, fill = "blue") +
  geom_vline(aes(xintercept = -0.134), colour = "blue") +
  theme_classic() + labs(x = "Weber Contrast") + 
  scale_y_continuous("Predicted Shadow Responses",
                     sec.axis = sec_axis(~ . * 100,
                                         name = "Shadow Responses Elicited (%)"))
```


### Why the data are more extreme than the predictions?

Well done on the plot. It is not unheard of for some of the raw data points to be more extreme than the predictions. There are several reasons for this. 

* The shaded range for each is a 95% confidence interval for the fitted line. You can change the *alpha* of the confidence interval above using the ci.lvl attribute in ggpredict to anything you want, though 0.95 is conventional. Values would therefore be expected to exceed this range 95% of the time.

* The model should correspond to the data but should not recapitulate it perfectly - otherwise there would be no point running a model - you would just plot the raw data. While the raw data are in a way more *real*, the model has something the raw data lack: The likelihood function (in this case, the binomial distribution) which corresponds to how the data were made and understands what is likely and what is not.

* Specifically, this plot of the model and the set of predictions used is based only on the *fixed* effects and leaves out the *random* effects, which include the variation explained by individual differences. Having random effects in your model leads to *shrinkage* and more certain estimates by trusting less the more extreme values and more extreme individuals (or other such groups). You can also make predictions with the random effects in, which is sometimes called the prediction interval. The prediction interval will necessarily be wider because it includes this extra variation. Unfortunately, ggpredict throws an error and will not let me plot prediction intervals for this model (by setting type="random"). Commonly, we leave this out because we are not usually interested in these specific chitons but rather the population of all chitons. The raw data, however, includes this extra variation that you have taken out and will be more extreme. Ideally, you would have an extra pair of lines showing the prediction interval that would include more of the data proportions.

* I have so far talked about non-systemic fluctuations away from the fitted line in either direction - but you definitely have a systemic difference, in that the highest proportions for each treatment - corresponding to the more salient stimuli - fall below the model. This means that actually there is a lapse rate for the chitons: A rate at which the chitons fail even with a really clear stimulus.  You can estimate this rate by getting the success proportion for each treatment with the most salient stimulus (-0.95 Weber contrast) - it appears to be 0.05 in each case. (It may be lower in the case of the girdle where the most salient stimulus level conflicts slightly with the three next most salient stimulus levels - unless they have some weird preference for specific contrast levels this looks to me to be at least partly just random variation). You have too options here: You can explain that your model is slightly askew because of this lapse (and therefore the *true *threshold may be at a slightly more negative value than the fitted line) or you can introduce an upper asymptote to consider the lapse rate.  

### A few other changes

* I made the range of the plot along the x-axis wider. It now includes values outside the range of your measurements, which (in the case of values of stimulus above 0) do not correspond to the real world but do tell you how the model *thinks*.

* I changed Weber__'s__ to *Weber*

* I made the other plot's colours correspond.

* I changed the appearance of the raw data points using pch so you can see all of those for the girdle.

### Notes

* You will need to add a legend to the plot at some point

* Consider continuing to use this Rmardown format to run your code (you add R code chunks with Ctrl + Alt + I). It allows you to mix code, text and figures in one go and can easily be made into a conventient document by pressing Crtl + Shift + K or the knit button above. It is easy to follow for anyone who wants to read it which is good for reproducibility and to learn from. It's also simpler for me : )

### Plot random effects for some individuals

```{r}
set.seed(321)

x_prediction_sequence <- seq(from=-1, to=0.2, by= 0.005)

predz <- ggpredict(
  compmodel.intslope, 
  terms = c('stimulus [x_prediction_sequence]','individual [sample=9]','treatment'),
  type='re', # can also be random
  ci.lvl = 0.95, # 1- alpha (usually 0.05) level of confidence intervals
                   ) 
predz %>% plot() #colors=c("blue","black","orange")
```

### Or the other way around

```{r}
set.seed(321)

x_prediction_sequence <- seq(from=-1, to=0.2, by= 0.005)

predz <- ggpredict(
  compmodel.intslope, 
  terms = c('stimulus [x_prediction_sequence]','treatment','individual [sample=9]'),
  type='re', # can also be random
  ci.lvl = 0.95, # 1- alpha (usually 0.05) level of confidence intervals
                   ) 
predz %>% plot() #colors=c("blue","black","orange")
```


